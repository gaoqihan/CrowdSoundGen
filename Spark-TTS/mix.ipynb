{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qihan/miniconda3/envs/sparktts/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import platform\n",
    "import sys\n",
    "import random\n",
    "sys.path.append(os.path.abspath(\"./cli\"))\n",
    "sys.path.append(os.path.abspath(\"./sparktts\"))\n",
    "from sparktts.utils.token_parser import LEVELS_MAP,LEVELS_MAP_UI, GENDER_MAP, TASK_TOKEN_MAP\n",
    "\n",
    "\n",
    "from SparkTTS import SparkTTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def initialize_model(model_dir=\"pretrained_models/Spark-TTS-0.5B\", device=0):\n",
    "    \"\"\"Load the model once at the beginning.\"\"\"\n",
    "\n",
    "    # Determine appropriate device based on platform and availability\n",
    "    if platform.system() == \"Darwin\":\n",
    "        # macOS with MPS support (Apple Silicon)\n",
    "        device = torch.device(f\"mps:{device}\")\n",
    "    elif torch.cuda.is_available():\n",
    "        # System with CUDA support\n",
    "        device = torch.device(f\"cuda:{device}\")\n",
    "    else:\n",
    "        # Fall back to CPU\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model = SparkTTS(model_dir, device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_tts(\n",
    "    text,\n",
    "    model,\n",
    "    prompt_text=None,\n",
    "    prompt_speech=None,\n",
    "    gender=None,\n",
    "    pitch=None,\n",
    "    speed=None,\n",
    "    save_dir=\"example/results\",\n",
    "):\n",
    "    \"\"\"Perform TTS inference and save the generated audio.\"\"\"\n",
    "\n",
    "    if prompt_text is not None:\n",
    "        prompt_text = None if len(prompt_text) <= 1 else prompt_text\n",
    "\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Generate unique filename using timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    save_path = os.path.join(save_dir, f\"{timestamp}.wav\")\n",
    "\n",
    "\n",
    "    # Perform inference and save the output audio\n",
    "    with torch.no_grad():\n",
    "        wav = model.inference(\n",
    "            text,\n",
    "            prompt_speech,\n",
    "            prompt_text,\n",
    "            gender,\n",
    "            pitch,\n",
    "            speed,\n",
    "        )\n",
    "\n",
    "        sf.write(save_path, wav, samplerate=16000)\n",
    "\n",
    "    return save_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text=\"Happy Birthday Jeff!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qihan/miniconda3/envs/sparktts/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing tensor: mel_transformer.spectrogram.window\n",
      "Missing tensor: mel_transformer.mel_scale.fb\n"
     ]
    }
   ],
   "source": [
    "model=initialize_model()\n",
    "save_dir=f\"example/results/{input_text.replace(' ', '_')}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "#N= number of sound source\n",
    "\n",
    "\n",
    "N=5\n",
    "random_list_pitch = [random.randint(4, 4) for _ in range(N)]\n",
    "random_list_speed = [random.randint(1,5) for _ in range(N)]\n",
    "random_list_gender = [random.randint(1, 2) for _ in range(N)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "GENDER_MAP_UI={1:\"female\",2:\"male\"}\n",
    "for i in range(N):\n",
    "\n",
    "    run_tts(\n",
    "        input_text,\n",
    "        model,\n",
    "        save_dir=save_dir,\n",
    "        gender=GENDER_MAP_UI[random_list_gender[i]],\n",
    "        pitch=LEVELS_MAP_UI[random_list_pitch[i]],\n",
    "        speed=LEVELS_MAP_UI[random_list_speed[i]],\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply additional variation to the sound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized loudness list: [8, 9, 10, 3, 7, 7, 0]\n",
      "Overlayed audio 20250328234404.wav - Duration: 2.48 seconds, Loudness Adjustment: 8 dB\n",
      "Overlayed audio 20250328235256.wav - Duration: 2.48 seconds, Loudness Adjustment: 9 dB\n",
      "Overlayed audio 20250328235257.wav - Duration: 2.48 seconds, Loudness Adjustment: 10 dB\n",
      "Overlayed audio 20250328234405.wav - Duration: 2.48 seconds, Loudness Adjustment: 3 dB\n",
      "Overlayed audio 20250328235258.wav - Duration: 2.48 seconds, Loudness Adjustment: 7 dB\n",
      "Overlayed audio 20250328234407.wav - Duration: 2.48 seconds, Loudness Adjustment: 7 dB\n",
      "Overlayed audio 20250328234406.wav - Duration: 2.48 seconds, Loudness Adjustment: 0 dB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='example/results/Happy_Birthday_Jeff!/final_output.wav'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Function to normalize audio\n",
    "def normalize_audio(audio, target_dBFS=-20.0):\n",
    "    change_in_dBFS = target_dBFS - audio.dBFS\n",
    "    return audio.apply_gain(change_in_dBFS)\n",
    "\n",
    "# Normalize the loudness list to a range of -10 to 10 dB\n",
    "def normalize_loudness(loudness_list):\n",
    "    max_value = max(abs(x) for x in loudness_list)\n",
    "    if max_value == 0:\n",
    "        return [0] * len(loudness_list)  # All zeros if max_value is zero\n",
    "    return [int((x / max_value) * 10) for x in loudness_list]\n",
    "\n",
    "# Directory containing the WAV files\n",
    "files = [f for f in os.listdir(save_dir) if f.endswith('.wav')]\n",
    "loudness_list = [random.randint(1, 100) for _ in range(len(files))]\n",
    "# Example loudness list (replace with your own values)\n",
    "\n",
    "if len(loudness_list) != len(files):\n",
    "    raise ValueError(\"Length of loudness list must match the number of audio files.\")\n",
    "\n",
    "# Normalize the loudness list\n",
    "normalized_loudness_list = normalize_loudness(loudness_list)\n",
    "print(f\"Normalized loudness list: {normalized_loudness_list}\")\n",
    "\n",
    "# Determine the maximum duration among all audio files\n",
    "max_duration = 0\n",
    "audio_segments = []\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(save_dir, file)\n",
    "    audio = AudioSegment.from_wav(file_path)\n",
    "    audio_segments.append(audio)\n",
    "    max_duration = max(max_duration, len(audio))  # Length is in milliseconds\n",
    "\n",
    "# Initialize a silent audio segment with the maximum duration\n",
    "final_audio = AudioSegment.silent(duration=max_duration)\n",
    "\n",
    "# Loop through all audio files and overlay them on the final audio\n",
    "for i, audio in enumerate(audio_segments):\n",
    "    normalized_audio = normalize_audio(audio)\n",
    "    # Apply the relative loudness adjustment based on the normalized loudness list\n",
    "    adjusted_audio = normalized_audio + normalized_loudness_list[i]\n",
    "    final_audio = final_audio.overlay(adjusted_audio)\n",
    "    print(f\"Overlayed audio {files[i]} - Duration: {final_audio.duration_seconds} seconds, Loudness Adjustment: {normalized_loudness_list[i]} dB\")\n",
    "\n",
    "# Export the final mixed audio\n",
    "final_audio.export(os.path.join(save_dir, \"final_output.wav\"), format=\"wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to match features using DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.       0.       0.999999 ... 1.       1.       1.      ]\n",
      "Audio 20250328235256.wav - Calculated Local Speed Ratios: [1.       0.       0.999999 0.999999 0.999999 0.999999 0.999999 0.\n",
      " 0.       0.999999]...\n",
      "len audio 33600\n",
      "len audio 33600\n",
      "len audio 33600\n",
      "[1.       0.999999 0.999999 ... 1.       1.       1.      ]\n",
      "Audio final_output.wav - Calculated Local Speed Ratios: [1.       0.999999 0.999999 0.999999 0.       0.       0.       0.999999\n",
      " 0.       0.999999]...\n",
      "len audio 39680\n",
      "len audio 39680\n",
      "len audio 39680\n",
      "[1.       0.999999 0.999999 ... 1.       1.       1.      ]\n",
      "Audio 20250328235257.wav - Calculated Local Speed Ratios: [1.       0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.\n",
      " 0.       0.      ]...\n",
      "len audio 19200\n",
      "len audio 19200\n",
      "[1.       0.999999 0.999999 ... 1.       1.       1.      ]\n",
      "Audio 20250328234405.wav - Calculated Local Speed Ratios: [1.       0.999999 0.999999 0.999999 0.999999 0.999999 0.       0.\n",
      " 0.       0.      ]...\n",
      "len audio 16000\n",
      "len audio 16000\n",
      "[1.       0.999999 0.       ... 1.       1.       1.      ]\n",
      "Audio 20250328235258.wav - Calculated Local Speed Ratios: [1.       0.999999 0.       0.       0.       0.       0.999999 0.\n",
      " 0.       0.      ]...\n",
      "len audio 39680\n",
      "len audio 39680\n",
      "len audio 39680\n",
      "[1.       0.999999 0.999999 ... 1.       1.       1.      ]\n",
      "Audio 20250328234407.wav - Calculated Local Speed Ratios: [1.       0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999]...\n",
      "len audio 16320\n",
      "len audio 16320\n",
      "[1.       0.999999 0.999999 ... 1.       1.       1.      ]\n",
      "Audio 20250328234406.wav - Calculated Local Speed Ratios: [1.       0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999]...\n",
      "len audio 16320\n",
      "len audio 16320\n",
      "Overlayed audio 20250328234404.wav - Duration: 2.483 seconds, Loudness Adjustment: 9 dB\n",
      "Overlayed audio 20250328235256.wav - Duration: 2.483 seconds, Loudness Adjustment: 9 dB\n",
      "Overlayed audio final_output.wav - Duration: 2.483 seconds, Loudness Adjustment: 8 dB\n",
      "Overlayed audio 20250328235257.wav - Duration: 2.483 seconds, Loudness Adjustment: 6 dB\n",
      "Overlayed audio 20250328234405.wav - Duration: 2.483 seconds, Loudness Adjustment: 2 dB\n",
      "Overlayed audio 20250328235258.wav - Duration: 2.483 seconds, Loudness Adjustment: 10 dB\n",
      "Overlayed audio 20250328234407.wav - Duration: 2.483 seconds, Loudness Adjustment: 3 dB\n",
      "Overlayed audio 20250328234406.wav - Duration: 2.483 seconds, Loudness Adjustment: 3 dB\n",
      "Final mixed audio saved to: example/results/Happy_Birthday_Jeff!/final_output.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qihan/miniconda3/envs/sparktts/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1904\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Normalize audio loudness\n",
    "def normalize_audio(audio, target_dBFS=-20.0):\n",
    "    change_in_dBFS = target_dBFS - audio.dBFS\n",
    "    return audio.apply_gain(change_in_dBFS)\n",
    "\n",
    "# Calculate dynamic local speed ratios using DTW\n",
    "def calculate_local_speed_ratios(reference, target, sr):\n",
    "    ref_mel = librosa.feature.melspectrogram(y=reference, sr=sr, n_mels=128)\n",
    "    tar_mel = librosa.feature.melspectrogram(y=target, sr=sr, n_mels=128)\n",
    "    distance, path = fastdtw(ref_mel.T, tar_mel.T, dist=euclidean)\n",
    "    path = np.array(path)\n",
    "\n",
    "    # Calculate local speed ratios (slope between consecutive points)\n",
    "    speed_ratios = np.ones(len(target))\n",
    "    for i in range(1, len(path)):\n",
    "        ref_step = path[i, 0] - path[i - 1, 0]\n",
    "        tar_step = path[i, 1] - path[i - 1, 1]\n",
    "        if tar_step > 0:\n",
    "            local_speed = ref_step / (tar_step + 1e-6)\n",
    "            speed_ratios[path[i, 1]] = local_speed\n",
    "    print(speed_ratios)\n",
    "    return speed_ratios\n",
    "\n",
    "\n",
    "# Adjust local speed dynamically\n",
    "def adjust_local_speed(audio, speed_ratios, sr):\n",
    "    adjusted_audio = np.array([], dtype=np.float32)\n",
    "    segment_length = 14096  # Frame length for adjustment\n",
    "\n",
    "    for i in range(0, len(audio), segment_length):\n",
    "        print(\"len audio\",len(audio))\n",
    "        segment = audio[i:i + segment_length]\n",
    "        speed_ratio = np.mean(speed_ratios[i:i + segment_length])\n",
    "        \n",
    "        # Avoid zero or negative speed ratios\n",
    "        if speed_ratio <= 0:\n",
    "            speed_ratio = 1.0\n",
    "        \n",
    "        # Ensure the segment is long enough to time-stretch\n",
    "        if len(segment) > 1:\n",
    "            # Adjust the speed of the entire segment\n",
    "            stretched_segment = librosa.effects.time_stretch(segment.astype(np.float32), rate=speed_ratio)\n",
    "        else:\n",
    "            stretched_segment = segment\n",
    "\n",
    "        # Concatenate the adjusted segment to the output audio\n",
    "        adjusted_audio = np.concatenate((adjusted_audio, stretched_segment))\n",
    "\n",
    "    return adjusted_audio\n",
    "\n",
    "# Directory containing the WAV files\n",
    "files = [f for f in os.listdir(save_dir) if f.endswith('.wav')]\n",
    "\n",
    "# Determine the maximum duration among all audio files\n",
    "audio_segments = []\n",
    "aligned_segments = []\n",
    "\n",
    "# Load the first audio as the reference\n",
    "reference_audio_path = os.path.join(save_dir, files[0])\n",
    "reference_audio, sample_rate = librosa.load(reference_audio_path, sr=None)\n",
    "audio_segments.append(reference_audio)\n",
    "\n",
    "# Load and normalize other audios\n",
    "for file in files[1:]:\n",
    "    file_path = os.path.join(save_dir, file)\n",
    "    audio, sr = librosa.load(file_path, sr=sample_rate)\n",
    "    audio_segments.append(audio)\n",
    "\n",
    "# Align all other audios to the reference by dynamically adjusting local speed\n",
    "aligned_segments = []\n",
    "\n",
    "for i, samples in enumerate(audio_segments):\n",
    "    if i == 0:\n",
    "        aligned_segments.append(samples)\n",
    "        continue\n",
    "\n",
    "    # Calculate local speed ratios using DTW\n",
    "    speed_ratios = calculate_local_speed_ratios(reference_audio, samples, sample_rate)\n",
    "    print(f\"Audio {files[i]} - Calculated Local Speed Ratios: {speed_ratios[:10]}...\")\n",
    "\n",
    "    # Adjust local speed dynamically\n",
    "    adjusted_samples = adjust_local_speed(samples, speed_ratios, sample_rate)\n",
    "    aligned_segments.append(adjusted_samples)\n",
    "\n",
    "# Convert aligned audio segments to pydub AudioSegment objects\n",
    "audio_segments = [\n",
    "    AudioSegment(\n",
    "        (aligned * (2**15)).astype(np.int16).tobytes(),\n",
    "        frame_rate=sample_rate,\n",
    "        sample_width=2,\n",
    "        channels=1\n",
    "    ) for aligned in aligned_segments\n",
    "]\n",
    "\n",
    "# Find the maximum duration among aligned segments\n",
    "max_duration = max(len(a) for a in audio_segments)\n",
    "\n",
    "# Initialize the final audio segment with silence of the maximum duration\n",
    "final_audio = AudioSegment.silent(duration=max_duration)\n",
    "\n",
    "# Apply loudness normalization to each segment\n",
    "loudness_list = [random.randint(1, 100) for _ in range(len(audio_segments))]\n",
    "\n",
    "# Normalize loudness list to range of -10 to 10 dB\n",
    "def normalize_loudness(loudness_list):\n",
    "    max_value = max(abs(x) for x in loudness_list)\n",
    "    if max_value == 0:\n",
    "        return [0] * len(loudness_list)\n",
    "    return [int((x / max_value) * 10) for x in loudness_list]\n",
    "\n",
    "normalized_loudness_list = normalize_loudness(loudness_list)\n",
    "\n",
    "# Overlay the aligned audios with loudness adjustment\n",
    "for i, audio in enumerate(audio_segments):\n",
    "    normalized_audio = normalize_audio(audio)\n",
    "    adjusted_audio = normalized_audio + normalized_loudness_list[i]\n",
    "    # Accumulate overlays by mixing with the final_audio\n",
    "    final_audio = final_audio.overlay(adjusted_audio, position=0)\n",
    "    print(f\"Overlayed audio {files[i]} - Duration: {final_audio.duration_seconds} seconds, Loudness Adjustment: {normalized_loudness_list[i]} dB\")\n",
    "\n",
    "# Export the final mixed audio\n",
    "final_output_path = os.path.join(save_dir, \"final_output.wav\")\n",
    "final_audio.export(final_output_path, format=\"wav\")\n",
    "print(f\"Final mixed audio saved to: {final_output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparktts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
